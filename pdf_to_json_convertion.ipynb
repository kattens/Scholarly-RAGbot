{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1-f5_WI1VdMFEiu0yOrex_xlpv_MJjmMh",
      "authorship_tag": "ABX9TyN27GBR8T4Er0FgTYoG+9TM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kattens/Scholarly-RAGbot/blob/main/pdf_to_json_convertion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First Step:  \n",
        "We will begin by creating a database of local papers by converting all available PDFs into a single JSON file. This initial approach can later be enhanced with SQL or other database management systems for improved handling.\n",
        "\n",
        "we extract the **title, keywords, abstract, and DOI** from your PDFs and store them in a JSON file for easy searching.\n",
        "\n",
        "### **Steps:**\n",
        "1. **Extract text** from PDFs.\n",
        "2. **Parse metadata** (title, keywords, abstract, DOI).\n",
        "3. **Store data in JSON**.\n",
        "\n",
        "\n",
        "- our objective is to take an OOP approach"
      ],
      "metadata": {
        "id": "HWLmazmJFQ3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for pdf modification\n",
        "!pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpVwMeeEF2Oj",
        "outputId": "26577fa4-4a55-4f13-9278-279a66d42178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWmR3u5dFF0f"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import fitz #pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "class PDFProcessor:\n",
        "    \"\"\"Class to process PDFs and extract metadata like title, abstract, keywords, and DOI.\"\"\"\n",
        "\n",
        "    def __init__(self, pdf_folder, output_folder, output_json=\"papers_metadata.json\"):\n",
        "        \"\"\"\n",
        "        Initializes the PDFProcessor with a folder containing PDFs and an output JSON file.\n",
        "\n",
        "        :param pdf_folder: Directory containing PDF files.\n",
        "        :param output_folder: Directory where the output JSON file will be saved.\n",
        "        :param output_json: JSON file to store extracted metadata.\n",
        "        \"\"\"\n",
        "        self.pdf_folder = pdf_folder\n",
        "        self.output_folder = output_folder\n",
        "        self.output_json = os.path.join(output_folder, output_json)\n",
        "        self.papers = []\n",
        "\n",
        "        # Ensure output folder exists\n",
        "        os.makedirs(self.output_folder, exist_ok=True)\n",
        "\n",
        "    def extract_text(self, pdf_path):\n",
        "        \"\"\"\n",
        "        Extracts text from a PDF file.\n",
        "\n",
        "        :param pdf_path: Path to the PDF file.\n",
        "        :return: Extracted text as a string.\n",
        "        \"\"\"\n",
        "        doc = fitz.open(pdf_path)\n",
        "        return \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
        "\n",
        "    def extract_metadata(self, text):\n",
        "        \"\"\"\n",
        "        Extracts metadata (title, keywords, abstract, DOI) from the text.\n",
        "\n",
        "        :param text: Extracted text from the PDF.\n",
        "        :return: Dictionary containing metadata.\n",
        "        \"\"\"\n",
        "        metadata = {}\n",
        "\n",
        "        # Extract Title (First non-empty line is assumed to be the title)\n",
        "        lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
        "        metadata[\"title\"] = lines[0] if lines else \"Unknown Title\"\n",
        "\n",
        "        # Extract DOI using regex\n",
        "        doi_match = re.search(r\"10\\.\\d{4,9}/[-._;()/:A-Za-z0-9]+\", text)\n",
        "        metadata[\"doi\"] = doi_match.group(0) if doi_match else \"Unknown DOI\"\n",
        "\n",
        "        # Extract Abstract (Look for the keyword \"Abstract\" and capture multiline text)\n",
        "        abstract_match = re.search(r\"\\bAbstract\\b[:\\s]*([\\s\\S]+?)(?=\\n[A-Z])\", text, re.IGNORECASE)\n",
        "        metadata[\"abstract\"] = abstract_match.group(1).strip() if abstract_match else \"No abstract found\"\n",
        "\n",
        "        # Extract Keywords using TF-IDF\n",
        "        metadata[\"keywords\"] = self.extract_keywords(text)\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    def extract_keywords(self, text, num_keywords=10):\n",
        "        \"\"\"\n",
        "        Extracts top keywords from the text using TF-IDF.\n",
        "\n",
        "        :param text: Extracted text from the PDF.\n",
        "        :param num_keywords: Number of keywords to extract.\n",
        "        :return: List of extracted keywords.\n",
        "        \"\"\"\n",
        "        # Preprocess text: Remove numbers and special characters\n",
        "        clean_text = re.sub(r'\\W+', ' ', text.lower())\n",
        "\n",
        "        # Use TF-IDF to extract keywords\n",
        "        vectorizer = TfidfVectorizer(max_features=num_keywords, stop_words=\"english\")\n",
        "        tfidf_matrix = vectorizer.fit_transform([clean_text])\n",
        "        keywords = vectorizer.get_feature_names_out()\n",
        "\n",
        "        return list(keywords)\n",
        "\n",
        "    def process_pdfs(self):\n",
        "        \"\"\"\n",
        "        Processes all PDFs in the specified folder and extracts metadata.\n",
        "        \"\"\"\n",
        "        for filename in os.listdir(self.pdf_folder):\n",
        "            if filename.endswith(\".pdf\"):\n",
        "                pdf_path = os.path.join(self.pdf_folder, filename)\n",
        "                print(f\"Processing {filename}...\")\n",
        "\n",
        "                text = self.extract_text(pdf_path)\n",
        "                metadata = self.extract_metadata(text)\n",
        "                metadata[\"file\"] = filename  # Add filename for reference\n",
        "                self.papers.append(metadata)\n",
        "\n",
        "        self.save_to_json()\n",
        "        print(f\"Metadata saved to {self.output_json}\")\n",
        "\n",
        "    def save_to_json(self):\n",
        "        \"\"\"\n",
        "        Saves extracted metadata to a JSON file.\n",
        "        \"\"\"\n",
        "        with open(self.output_json, \"w\", encoding=\"utf-8\") as json_file:\n",
        "            json.dump(self.papers, json_file, indent=4, ensure_ascii=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "rXyVIaZRoRyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pdf_folder=\"/content/drive/MyDrive/papers/oldpapers/45NSYUH9\"\n",
        "output_folder = '/content/drive/MyDrive/papers/paperjson/'\n",
        "\n",
        "# Example usage:\n",
        "pdf_processor = PDFProcessor(pdf_folder, output_folder)\n",
        "pdf_processor.process_pdfs()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-38yyzEaoSSl",
        "outputId": "1c9724ec-370f-40f9-e30f-b01a1c108dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Ieremie et al. - 2022 - TransformerGO predicting protein–protein interact.pdf...\n",
            "Metadata saved to /content/drive/MyDrive/papers/paperjson/jsonfile1.json/papers_metadata.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pl-Eq_xzocMf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}